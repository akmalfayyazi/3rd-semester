{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Klasifikasi Limbah dengan Convolutional Neural Network (CNN)\n",
        "Pada tugas ini, kita akan menggunakan **RealWaste Dataset** yang dapat diakses melalui tautan berikut:\\\n",
        "ğŸ”— https://www.kaggle.com/datasets/joebeachcapital/realwaste/data\n",
        "\n",
        "Dataset ini berisi gambar limbah nyata dari lingkungan, dengan berbagai kategori jenis limbah. \n",
        "Tujuan utama dari tugas ini adalah membangun **model CNN (Convolutional Neural Network)** untuk mengklasifikasikan jenis limbah berdasarkan gambar.\n",
        "\n",
        "Langkah-langkah yang dilakukan adalah:\n",
        "1. Persiapan Dataset & Eksplorasi Awal\n",
        "2. Preprocessing Data (Resize, Augmentasi, Normalisasi)\n",
        "3. Membangun Model CNN\n",
        "4. Training dan Validasi Model\n",
        "5. Evaluasi dan Visualisasi\n",
        "6. Analisis dan Kesimpulan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.applications import EfficientNetB3\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Load Dataset & Explorasi Awal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4752 files belonging to 9 classes.\n",
            "Using 3802 files for training.\n",
            "Found 4752 files belonging to 9 classes.\n",
            "Using 950 files for validation.\n"
          ]
        }
      ],
      "source": [
        "# Set random seed untuk reprodusibilitas\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# ==================== 1. PERSIAPAN DATASET & EKSPLORASI AWAL ====================\n",
        "print(\"=\"*60)\n",
        "print(\"1. PERSIAPAN DATASET & EKSPLORASI AWAL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Path dataset\n",
        "train_dir = '/content/drive/MyDrive/Colab-Notebooks/sem3/CNN/train/'\n",
        "test_dir = '/content/drive/MyDrive/Colab-Notebooks/sem3/CNN/test/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fungsi untuk menghitung jumlah gambar per kategori\n",
        "def count_images_per_class(directory):\n",
        "    class_counts = {}\n",
        "    for class_name in os.listdir(directory):\n",
        "        class_path = os.path.join(directory, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            count = len([f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "            class_counts[class_name] = count\n",
        "    return class_counts\n",
        "\n",
        "# Hitung distribusi data training\n",
        "train_counts = count_images_per_class(train_dir)\n",
        "print(\"\\nğŸ“Š Distribusi Data Training:\")\n",
        "for class_name, count in sorted(train_counts.items()):\n",
        "    print(f\"  {class_name}: {count} images\")\n",
        "\n",
        "total_train = sum(train_counts.values())\n",
        "print(f\"\\nâœ… Total gambar training: {total_train}\")\n",
        "\n",
        "# Visualisasi distribusi kelas\n",
        "plt.figure(figsize=(12, 6))\n",
        "classes = list(train_counts.keys())\n",
        "counts = list(train_counts.values())\n",
        "bars = plt.bar(classes, counts, color='steelblue', edgecolor='black')\n",
        "plt.xlabel('Kategori Limbah', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Jumlah Gambar', fontsize=12, fontweight='bold')\n",
        "plt.title('Distribusi Data Training per Kategori', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Tambahkan nilai di atas bar\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{int(height)}',\n",
        "             ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualisasi contoh gambar dari tiap kategori\n",
        "print(\"\\nğŸ–¼ï¸ Visualisasi Contoh Gambar per Kategori:\")\n",
        "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, class_name in enumerate(sorted(classes)):\n",
        "    class_path = os.path.join(train_dir, class_name)\n",
        "    images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    if images:\n",
        "        img_path = os.path.join(class_path, images[0])\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        axes[idx].imshow(img)\n",
        "        axes[idx].set_title(f'{class_name}\\n({train_counts[class_name]} images)', fontsize=11, fontweight='bold')\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Preprocessing & Augmentasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "\n",
        "# Data augmentation untuk training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2  # 80% train, 20% validation\n",
        ")\n",
        "\n",
        "# Data generator untuk validation (hanya rescaling)\n",
        "val_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Load training data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Load validation data\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=False,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Simpan class indices untuk prediksi nanti\n",
        "class_indices = train_generator.class_indices\n",
        "class_names = {v: k for k, v in class_indices.items()}\n",
        "\n",
        "print(f\"\\nâœ… Training samples: {train_generator.samples}\")\n",
        "print(f\"âœ… Validation samples: {validation_generator.samples}\")\n",
        "print(f\"\\nğŸ“‹ Class Mapping:\")\n",
        "for idx, name in class_names.items():\n",
        "    print(f\"  {idx}: {name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Membangun Model CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model(num_classes):\n",
        "    # Menggunakan EfficientNetB3 sebagai base model (transfer learning)\n",
        "    base_model = EfficientNetB3(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
        "    )\n",
        "\n",
        "    # Freeze base model layers untuk fine-tuning bertahap\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Build model\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model, base_model\n",
        "\n",
        "# Create model\n",
        "num_classes = len(class_indices)\n",
        "model, base_model = create_model(num_classes)\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "print(\"\\nğŸ“ Arsitektur Model:\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Training & Validasi Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m119/119\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.1888 - loss: 2.1187 - val_accuracy: 0.2484 - val_loss: 2.0387\n",
            "Epoch 2/10\n",
            "\u001b[1m119/119\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 2s/step - accuracy: 0.2438 - loss: 1.9556 - val_accuracy: 0.2832 - val_loss: 2.0875\n",
            "Epoch 3/10\n",
            "\u001b[1m119/119\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 2s/step - accuracy: 0.3077 - loss: 1.8574 - val_accuracy: 0.3937 - val_loss: 1.6076\n",
            "Epoch 4/10\n",
            "\u001b[1m119/119\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 2s/step - accuracy: 0.3588 - loss: 1.7407 - val_accuracy: 0.4516 - val_loss: 1.6239\n",
            "Epoch 5/10\n",
            "\u001b[1m119/119\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 2s/step - accuracy: 0.3958 - loss: 1.6862 - val_accuracy: 0.4789 - val_loss: 1.4737\n",
            "Epoch 6/10\n",
            "\u001b[1m119/119\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 2s/step - accuracy: 0.3940 - loss: 1.6373 - val_accuracy: 0.4716 - val_loss: 1.4620\n",
            "Epoch 7/10\n",
            "\u001b[1m119/119\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 2s/step - accuracy: 0.4237 - loss: 1.5910 - val_accuracy: 0.4726 - val_loss: 1.4560\n",
            "Epoch 8/10\n",
            "\u001b[1m119/119\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 2s/step - accuracy: 0.4116 - loss: 1.5831 - val_accuracy: 0.4558 - val_loss: 1.4706\n",
            "Epoch 9/10\n",
            "\u001b[1m119/119\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 3s/step - accuracy: 0.4356 - loss: 1.5353 - val_accuracy: 0.4989 - val_loss: 1.3653\n",
            "Epoch 10/10\n",
            "\u001b[1m119/119\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 2s/step - accuracy: 0.4574 - loss: 1.4910 - val_accuracy: 0.4505 - val_loss: 1.5041\n"
          ]
        }
      ],
      "source": [
        "# Callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        'best_model.h5',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# Training fase 1: Train dengan base model frozen\n",
        "print(\"\\nğŸš€ Training Fase 1: Base Model Frozen\")\n",
        "history1 = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=25,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fine-tuning: Unfreeze beberapa layer terakhir dari base model\n",
        "print(\"\\nğŸ”¥ Training Fase 2: Fine-tuning (Unfreeze base model)\")\n",
        "base_model.trainable = True\n",
        "\n",
        "# Freeze layer awal, hanya train layer akhir\n",
        "for layer in base_model.layers[:-50]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Recompile dengan learning rate lebih kecil\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
        ")\n",
        "\n",
        "# Continue training\n",
        "history2 = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=25,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Gabungkan history dari kedua fase\n",
        "history = {\n",
        "    'loss': history1.history['loss'] + history2.history['loss'],\n",
        "    'val_loss': history1.history['val_loss'] + history2.history['val_loss'],\n",
        "    'accuracy': history1.history['accuracy'] + history2.history['accuracy'],\n",
        "    'val_accuracy': history1.history['val_accuracy'] + history2.history['val_accuracy']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Evaluasi & Visualisasi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m30/30\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 349ms/step - accuracy: 0.4505 - loss: 1.5041\n",
            "Test Loss: 1.5040987730026245\n",
            "Test Accuracy: 0.45052632689476013\n"
          ]
        }
      ],
      "source": [
        "# Load best model\n",
        "model.load_weights('best_model.h5')\n",
        "\n",
        "# Evaluasi pada validation set\n",
        "print(\"\\nğŸ“Š Evaluasi pada Validation Set:\")\n",
        "val_loss, val_accuracy, val_precision, val_recall = model.evaluate(validation_generator)\n",
        "val_f1 = 2 * (val_precision * val_recall) / (val_precision + val_recall)\n",
        "\n",
        "print(f\"\\nâœ… Validation Accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"âœ… Validation Precision: {val_precision*100:.2f}%\")\n",
        "print(f\"âœ… Validation Recall: {val_recall*100:.2f}%\")\n",
        "print(f\"âœ… Validation F1-Score: {val_f1*100:.2f}%\")\n",
        "\n",
        "# Prediksi pada validation set untuk confusion matrix\n",
        "validation_generator.reset()\n",
        "y_pred = model.predict(validation_generator, verbose=1)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = validation_generator.classes\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nğŸ“‹ Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes,\n",
        "                          target_names=list(class_names.values()),\n",
        "                          digits=4))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fungsi untuk prediksi gambar individual\n",
        "def predict_image(img_path, model, class_names):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "    img = img / 255.0\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "\n",
        "    prediction = model.predict(img, verbose=0)\n",
        "    predicted_class = np.argmax(prediction[0])\n",
        "    confidence = prediction[0][predicted_class]\n",
        "\n",
        "    return class_names[predicted_class], confidence\n",
        "\n",
        "# Ambil semua file gambar dari folder test\n",
        "test_images = []\n",
        "for file in os.listdir(test_dir):\n",
        "    if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "        test_images.append(file)\n",
        "\n",
        "print(f\"\\nğŸ“ Total gambar test: {len(test_images)}\")\n",
        "\n",
        "# Prediksi semua gambar test\n",
        "results = []\n",
        "print(\"\\nğŸ”® Melakukan prediksi...\")\n",
        "\n",
        "for img_file in test_images:\n",
        "    img_path = os.path.join(test_dir, img_file)\n",
        "    pred_label, confidence = predict_image(img_path, model, class_names)\n",
        "    results.append({\n",
        "        'filename': img_file,\n",
        "        'label': pred_label,\n",
        "        'confidence': confidence\n",
        "    })\n",
        "\n",
        "# Buat DataFrame\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "# Save ke CSV (format yang diminta)\n",
        "output_csv = 'predictions.csv'\n",
        "df_results[['filename', 'label']].to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"\\nâœ… Prediksi selesai! File disimpan di: {output_csv}\")\n",
        "print(f\"\\nğŸ“Š Preview hasil prediksi:\")\n",
        "print(df_results.head(10))\n",
        "\n",
        "# Statistik prediksi\n",
        "print(f\"\\nğŸ“ˆ Distribusi Prediksi Test Set:\")\n",
        "pred_counts = df_results['label'].value_counts()\n",
        "for label, count in pred_counts.items():\n",
        "    print(f\"  {label}: {count} images\")\n",
        "\n",
        "# Visualisasi distribusi prediksi test\n",
        "plt.figure(figsize=(12, 6))\n",
        "pred_counts.plot(kind='bar', color='coral', edgecolor='black')\n",
        "plt.xlabel('Kategori Limbah', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Jumlah Prediksi', fontsize=12, fontweight='bold')\n",
        "plt.title('Distribusi Prediksi pada Data Test', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualisasi beberapa contoh prediksi dari test set\n",
        "print(\"\\nğŸ–¼ï¸ Visualisasi Contoh Prediksi Test Set:\")\n",
        "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx in range(min(9, len(test_images))):\n",
        "    img_path = os.path.join(test_dir, test_images[idx])\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    pred_label = df_results.iloc[idx]['label']\n",
        "    confidence = df_results.iloc[idx]['confidence']\n",
        "\n",
        "    axes[idx].imshow(img)\n",
        "    axes[idx].set_title(f'{test_images[idx]}\\nPrediksi: {pred_label}\\nConfidence: {confidence*100:.1f}%',\n",
        "                       fontsize=9, fontweight='bold')\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Analisis & Kesimpulan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
